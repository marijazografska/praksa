# -*- coding: utf-8 -*-
"""Chapter2_praksa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1peo9GPMNKs5CMRLQiyqBvOxqey0BaCdT
"""

!pip install tiktoken

import torch

from importlib.metadata import version

print("torch version:", version("torch"))
print("tiktoken version:", version("tiktoken"))

text_path='/content/drive/MyDrive/the-Verdict.txt'
with open(text_path,'r') as f:
  text=f.read()

print(text)

len(text)

"""# Tokenizacija"""

import re
tokeniziran_tekst=re.split(r'([,.:;?_!"()\']|--|\s)', text)
tokeniziran_tekst=[zbor.strip() for zbor in tokeniziran_tekst if zbor.strip()]
tokeniziran_tekst

sorted_text=sorted(set(tokeniziran_tekst))

sorted_text

vocab={ word: integer for integer, word in enumerate(sorted_text)}

vocab

"""Sega pravam klasa za tokeniziranje sto posle ke ja povikuvame kako tokenizator za nekoja recenica"""

class MojTokenizator:
  def __init__(self,vocab):
    self.str_to_int=vocab
    self.int_to_str={i:s for s,i in vocab.items()}
  def encode(self,text):
    tokeniziran_tekst=re.split(r'([,.:;?_!"()\']|--|\s)', text)
    tokeniziran_tekst=[zbor.strip() for zbor in tokeniziran_tekst if zbor.strip()]
    ids=[self.str_to_int[i] for i in tokeniziran_tekst]
    return ids
  def decode(self,ids):
    text= " ".join([self.int_to_str[i] for i in ids])
    text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
    return text

#proba so zborovi sto postojat vo tekstot gore
tokenizator=MojTokenizator(vocab)
proba='I should have done a great thing.'
output=tokenizator.encode(proba)
output

proba2=[53, 879, 530, 360, 115, 508, 996, 7]
input=tokenizator.decode(proba2)
input

"""Ja prosiruvam klasata da prifakja slucai so nepoznati zborovi"""

sorted_text_unk=sorted(list(set(sorted_text)))
sorted_text_unk.extend(["<|endoftext|>", "<|unk|>"])
vocab={ word: integer for integer, word in enumerate(sorted_text_unk)}

vocab

class MojTokenizator:
  def __init__(self,vocab):
    self.str_to_int=vocab
    self.int_to_str={i:s for s,i in vocab.items()}
  def encode(self,text):
    tokeniziran_tekst=re.split(r'([,.:;?_!"()\']|--|\s)', text)
    tokeniziran_tekst=[zbor.strip() for zbor in tokeniziran_tekst if zbor.strip()]
    tokeniziran_tekst=[
          zbor if zbor in self.str_to_int
          else "<|unk|>" for zbor in tokeniziran_tekst
    ]
    ids=[self.str_to_int[i] for i in tokeniziran_tekst]

    return ids
  def decode(self,ids):
    text= " ".join([self.int_to_str[i] for i in ids])
    text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
    return text

#proba so zborovi sto postojat vo tekstot gore plus nepoznat zbor
tokenizator=MojTokenizator(vocab)
proba='I should have done a great Macedonian thing.'
output=tokenizator.encode(proba)
output

input=tokenizator.decode(output)
input

"""# Byte-Pair-Encoding"""

import tiktoken
bpe=tiktoken.get_encoding("gpt2")

proben_tekst="My name is Marija"

enkodiran_tekst=bpe.encode(proben_tekst)
enkodiran_tekst

for i in enkodiran_tekst:
  print(bpe.decode([i]))

"""Byte Pair encoder od HuggingFace

"""

import transformers

from transformers import GPT2Tokenizer
hf_bpe=GPT2Tokenizer.from_pretrained("gpt2")

enkodiran_tekst=hf_bpe.encode(proben_tekst)
enkodiran_tekst

for i in enkodiran_tekst:
  print(hf_bpe.decode([i]))

from transformers import T5Tokenizer

t5_bpe = T5Tokenizer.from_pretrained('t5-small')

enkodiran_tekst=t5_bpe.encode(proben_tekst, return_tensors='pt')
enkodiran_tekst

for i in enkodiran_tekst[0]:
  print(t5_bpe.decode([i]))

import torch
print("PyTorch version:", torch.__version__)

from torch.utils.data import Dataset, DataLoader

